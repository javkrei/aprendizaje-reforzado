{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El objetivo de este ejercicio es implementar q-learning y sarsa con aproximación de función de valor utilizando experience replay.\n",
    "\n",
    "## Experience replay consiste en lo siguiente:\n",
    "\n",
    "<img src=\"experience_replay.PNG\">\n",
    "\n",
    "## Recordar los targets de sarsa y q-learning respectivamente:\n",
    "\n",
    "## Sarsa:\n",
    "<img src=\"sarsa.PNG\">\n",
    "\n",
    "## Q-Learning:\n",
    "\n",
    "<img src=\"q-learning.PNG\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import sklearn.preprocessing\n",
    "from lib import plotting\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Red Neuronal que aproxima la función de valor estado acción (q-function)\n",
    "    \"\"\"   \n",
    "    def __init__(self, dimS, nA, learning_rate=0.001):\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(dimS,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(nA, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=learning_rate))\n",
    "     \n",
    "    def update(self, states, q_values, verbose=0):\n",
    "        \"\"\"\n",
    "        Realiza un update de los parámetros de la red neuronal usando un batch de estados y batch de vectores de valores\n",
    "        de la función estado-acción correspondientes\n",
    "        \"\"\"\n",
    "        self.model.fit(states, q_values, verbose=0)\n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Realiza una predicción de la función de valor estado-accion dado el estado\n",
    "        \n",
    "        Argumentos:\n",
    "            s: estado para el cual realizar la predicción\n",
    "            \n",
    "        Retorna:\n",
    "            Un vector con la predicción de la función de valor para todas las accións\n",
    "        \"\"\"\n",
    "        return self.model.predict(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, env, estimator, num_episodes, discount_factor=1.0, \n",
    "                 exploration_max=1.0, exploration_min=0.01, epsilon_decay=0.99, memory_size=1000000, \n",
    "                 batch_size=20, run_online=False, use_qlearning=True):\n",
    "        \"\"\"\n",
    "        Algoritmo q-learning/sarsa con experience replay utilizando aproximación de funciones.\n",
    "\n",
    "        Argumentos de inicialización:\n",
    "            env: ambiente de OpenAI.\n",
    "            estimator: función de aproximación de la función de valor estado-acción.\n",
    "            num_episodes: número de episodios durante los cuales entrenar el estimador.\n",
    "            discount_factor: factor de descuento gama.\n",
    "            exploration_max: probabilidad de tomar una acción aleatoria inicial.\n",
    "            exploration_min: mínima probabilidad de tomar una acción aleatoria.\n",
    "            epsilon_decay: en cada episodio la probabilidad de una acción aleatoria deace por este factor.\n",
    "        \"\"\"\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.env = env\n",
    "        self.estimator = estimator\n",
    "        self.num_episodes = num_episodes\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.run_online = run_online\n",
    "        self.use_qlearning = use_qlearning\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def decay_exploration_rate(self):\n",
    "        self.exploration_rate *= self.epsilon_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "        \n",
    "    def policy_fn(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Política epsilon-greedy basada en la función de aproximación actual y la probabilidad de exploración epsilon.\n",
    "\n",
    "        Retorna:\n",
    "            Un vector con la probabilidad de tomar cada acción.\n",
    "        \"\"\"\n",
    "        A = np.ones(env.action_space.n, dtype=float) * self.exploration_rate / env.action_space.n\n",
    "        q_values = self.estimator.predict(state)\n",
    "        best_action = np.argmax(q_values[0])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "        Realiza una corrida de entrenamiento de la función de aproximación dado un batch de experiencia acumulada.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # contienen un batch de 'experiencia' sampleada de la memoria acumulada\n",
    "        batch_q_values = np.zeros((self.batch_size, self.env.action_space.n))\n",
    "        batch_states = np.zeros((self.batch_size, self.env.observation_space.shape[0]))\n",
    "        \n",
    "        for i_batch, (state, action, reward, next_state, terminal) in enumerate(batch):\n",
    "            \n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                ## q-learning:\n",
    "                if self.use_qlearning:\n",
    "                    # COMPLETAR!\n",
    "                    # escribir el target de q-learning\n",
    "                    #q_update =\n",
    "                else: \n",
    "                ## sarsa:\n",
    "                    # COMPLETAR!\n",
    "                    # obtener las probabilidades de las acciones dado la política epsilon-greedy con\n",
    "                    # respecto a la función de valor estado-acción actual y el próximo estado\n",
    "                    # next_action_probs = \n",
    "                    # elegir la próxima acción aleatoriamente según esa distribución\n",
    "                    # next_action =\n",
    "                    # obtener los valores de la función de valor estado-acción para el próximo estado\n",
    "                    # q_values_next = \n",
    "                    # escribir el target de sarsa para la acción seleccionada\n",
    "                    # q_update = \n",
    "            \n",
    "            # COMPLETAR\n",
    "            # valores actuales de la función de valor para este estado:\n",
    "            # q_values = \n",
    "            # substituir el valor de la acción actual por el target\n",
    "            q_values[0][action] = q_update\n",
    "            \n",
    "            # llenar el array del batch\n",
    "            batch_q_values[i_batch] = q_values\n",
    "            batch_states[i_batch] = state\n",
    "            i_batch += 1\n",
    "        # avanzar en esa la dirección del gradiente dado los estados y los targets en el batch\n",
    "        self.estimator.update(batch_states, batch_q_values, verbose=0)\n",
    "        \n",
    "    def train(self):    \n",
    "        \"\"\"\n",
    "        Realiza Q-Learning o Sarsa con aproximación de la función de valor estado-acción.\n",
    "        Retorna:\n",
    "            Un objeto de tipo EpisodeStats con dos arrays de numpy para la longitud y recompensa acumulada de cada\n",
    "            episodio respectivamente.\n",
    "        \"\"\"\n",
    "        stats = plotting.EpisodeStats(\n",
    "            episode_lengths=np.zeros(self.num_episodes),\n",
    "            episode_rewards=np.zeros(self.num_episodes))    \n",
    "        \n",
    "        for run in range(self.num_episodes):\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, self.env.observation_space.shape[0]])\n",
    "            step = 0\n",
    "            cum_reward = 0.0\n",
    "            \n",
    "            action = np.random.choice(self.env.action_space.n, p=self.policy_fn(state, self.exploration_rate))\n",
    "            \n",
    "            while True:\n",
    "                env.render()\n",
    "                \n",
    "                next_state, reward, terminal, info = env.step(action)\n",
    "                \n",
    "                reward = reward if not terminal else -reward\n",
    "                cum_reward += reward*self.discount_factor**step\n",
    "                \n",
    "                next_state = np.reshape(next_state, [1, self.env.observation_space.shape[0]])\n",
    "                \n",
    "                if not terminal:\n",
    "                    next_action = np.random.choice(self.env.action_space.n, p=self.policy_fn(next_state, self.exploration_rate))\n",
    "                else:\n",
    "                    next_action = None\n",
    "                \n",
    "                \n",
    "                self.remember(state, action, reward, next_state, terminal)\n",
    "                \n",
    "                if self.run_online:\n",
    "                    # este es el caso en que hacemos un update en cada paso de experiencia:\n",
    "                    q_update = reward\n",
    "                    if not terminal:\n",
    "                        ## q-learning:\n",
    "                        if self.use_qlearning:\n",
    "                            # COMPLETAR!\n",
    "                            # escribir el target de q-learning\n",
    "                            # q_update = \n",
    "                        else: \n",
    "                        ## sarsa:\n",
    "                        # COMPLETAR!\n",
    "                            # obtener las probabilidades de las acciones dado la política epsilon-greedy con\n",
    "                            # respecto a la función de valor estado-acción actual y el próximo estado\n",
    "                            # next_action_probs = \n",
    "                            # elegir la próxima acción aleatoriamente según esa distribución\n",
    "                            # next_action =\n",
    "                            # obtener los valores de la función de valor estado-acción para el próximo estado\n",
    "                            # q_values_next = \n",
    "                            # escribir el target de sarsa para la acción seleccionada\n",
    "                            # q_update = \n",
    "                    # COMPLETAR!\n",
    "                    # valores actuales de la función de valor para este estado:\n",
    "                    # q_values = \n",
    "                    q_values[0][action] = q_update\n",
    "                    # hacer update en la dirección del gradiente\n",
    "                    self.estimator.update([state], [q_values], verbose=0)\n",
    "                    \n",
    "                \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                \n",
    "                step += 1\n",
    "                if terminal:\n",
    "                    print(\"Episodio: \" + str(run) + \", Exploración: \" + str(self.exploration_rate) + \n",
    "                          \", Recompensa Acumulada: \" + str(cum_reward))\n",
    "                    # Actualizar estadísticas\n",
    "                    stats.episode_rewards[run] = cum_reward\n",
    "                    stats.episode_lengths[run] = step\n",
    "                    break        \n",
    "                    \n",
    "                if not self.run_online:\n",
    "                    self.experience_replay()\n",
    "            self.decay_exploration_rate()\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(env.observation_space.shape[0], env.action_space.n)\n",
    "learner = Learner(env, estimator, 150, run_online=False, use_qlearning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats, smoothing_window=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
